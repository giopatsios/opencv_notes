{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abefd04-4eb1-43ec-a92a-4b73aad3696c",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49a701-a5e6-4e2a-b5cd-c25e9c5beb1e",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a274ee38-68dc-4560-9dd5-160d31b0e9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11185dea-80b7-4405-8ca0-07a17a80fb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "devide = torch.device ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f428f-8678-4573-aa9d-351c8852b1a4",
   "metadata": {},
   "source": [
    " > CIFAR-10 dataset consists of 60,000 32x32 colour images in 10 classes with 6000 images per class. There are 50,000 training images and 10,000 test images\n",
    " >> [CIFAR-10] https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c4a961-40fc-456d-b8d3-9cd6c835bd80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a115c3b-526f-416b-8a4b-7c7cbb2ba56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666dc9e-7a10-4ce7-94a9-125f0cf329b6",
   "metadata": {},
   "source": [
    "> Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5e006-bc83-424a-a291-464d48a05a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs=10\n",
    "num_classes=10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Download and load the pretrained ResNet-19\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "mode.to(device)\n",
    "\n",
    "# If you want to finetune the top layer of the model, set as below\n",
    "# for param in resenet.parameters():\n",
    "#     param.requires_grad=Flase\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Afam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972844b-4827-42a4-ab07-80d3bc2c18c5",
   "metadata": {},
   "source": [
    "> What is the ouput of a = [[[1, 2, 3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1c4469-508c-4f5f-8e57-1cdb0ba6fe47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[1, 2, 3]]]\n",
    "torch.tensor(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23589c5b-df70-451d-ad53-069f5f6a516a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[\n",
    "      2][1, 2],\n",
    "      [3,\n",
    "      ],\n",
    "     [\n",
    "      [1, 3],\n",
    "      [3, 4]\n",
    "     ] \n",
    "    ]\n",
    " \n",
    "torch.tensor(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0115bc71-b7ce-424f-a580-5ab5b995c79f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,2)\n",
    "b = torch.randn(3, 2)\n",
    "torch.vstack((a,b)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca2a79e-59f5-47d8-a614-d8ae263549fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    " \n",
    "torch.reshape(a, (-1, )).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "541ef01c-a273-480c-9bda-cfd202af02c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,3,4)\n",
    "torch.reshape(a, (2,12)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3131c2f-8a00-447b-b88d-0f974dda15ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[2, 4, 4], [3, 1, 9]])\n",
    "print(torch.argmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efd675b8-f01e-40e7-8ea0-c48a6c49bb55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[2, 4, 4], [3, 1, 9]])\n",
    "print(torch.argmax(a, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82aae618-8991-4a9a-ac4b-fac6f2dd2f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "a = torch.tensor(2., requires_grad=True)\n",
    " \n",
    "y = a*torch.pow(x, 2) \n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cbfe90c-d1ab-4ddc-8d27-7caaa3f098db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w1 = torch.tensor(2., requires_grad=True)\n",
    "b1 = torch.tensor(3., requires_grad=True)\n",
    "m = nn.ReLU()\n",
    "y = m(w1*x + b1)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4eb1b8ba-30a1-4274-abf0-e0b6a6ccb7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "a = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "c = torch.tensor(4., requires_grad=True)\n",
    " \n",
    "y = a*torch.pow(x, 2) + b * x + c\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
